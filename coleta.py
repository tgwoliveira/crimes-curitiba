import pandas as pd
import os
import requests
from bs4 import BeautifulSoup

# --- Configura√ß√µes da Base de Dados ---
# Fonte 1: Hist√≥rico (√≠ndice de arquivos)
BASE_URL_HISTORICO = "http://dadosabertos.c3sl.ufpr.br/curitiba/Sigesguarda/"
# Fonte 2: Mais Recente (p√°gina com link de download)
URL_DADOS_MAIS_RECENTES = "https://dadosabertos.curitiba.pr.gov.br/conjuntodado/detalhe?chave=b16ead9d-835e-41e8-a4d7-dcc4f2b4b627"
# -------------------------------------

# Pastas de sa√≠da
RAW_DIR = "data/raw"
PROCESSED_DIR = "data/processed"
os.makedirs(RAW_DIR, exist_ok=True)
os.makedirs(PROCESSED_DIR, exist_ok=True)

FINAL_CSV_PATH = os.path.join(PROCESSED_DIR, "ocorrencias_tratadas.csv")
primeiro_arquivo = True 


def get_historic_links():
    """Busca links de arquivos CSV no portal hist√≥rico."""
    csv_links = []
    try:
        response = requests.get(BASE_URL_HISTORICO)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        for link in soup.find_all('a'):
            href = link.get('href')
            if href and href.endswith('.csv') and 'sigesguarda' in href:
                csv_links.append(BASE_URL_HISTORICO + href)
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erro ao acessar o portal hist√≥rico: {e}")
    return csv_links


def get_latest_link():
    """Busca o link de download direto na p√°gina mais recente."""
    print("üåê Buscando link no portal de dados mais recentes...")
    try:
        response = requests.get(URL_DADOS_MAIS_RECENTES)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # O link de download geralmente tem a classe 'btn btn-primary' ou um atributo espec√≠fico.
        # Procuramos por um link que termine em .csv dentro da p√°gina.
        # Ajuste esta l√≥gica se n√£o funcionar, mas esta √© a tentativa mais robusta:
        for link in soup.find_all('a'):
            href = link.get('href')
            if href and 'ocorrencias-criminais.csv' in href and 'download' in href:
                # O link direto √© o valor do href
                return href
        
        # Se n√£o encontrar o link direto, tenta outra heur√≠stica comum de portais de dados abertos
        for link in soup.find_all('a', class_='btn-primary'):
            href = link.get('href')
            if href and 'ocorrencias-criminais' in href:
                return href
                
        print("‚ö†Ô∏è Link de download CSV n√£o encontrado na p√°gina mais recente.")
        return None

    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erro ao acessar o portal mais recente: {e}")
        return None


# =========================================================
# ETAPA 1: RASPAR TODAS AS FONTES DE DADOS
# =========================================================

# 1. Coleta Hist√≥rica
csv_links = get_historic_links()
print(f"‚úÖ {len(csv_links)} arquivos CSV encontrados na fonte hist√≥rica.")

# 2. Coleta Recente
latest_link = get_latest_link()
if latest_link and latest_link not in csv_links:
    csv_links.append(latest_link)
    print("‚úÖ Link mais recente adicionado para processamento.")

if not csv_links:
    print("‚ö†Ô∏è NENHUM link CSV v√°lido encontrado em nenhuma fonte.")
    exit()

# Ordenar a lista para garantir o processamento em ordem cronol√≥gica
csv_links.sort() 
print(f"\n‚úÖ Total de {len(csv_links)} arquivos √∫nicos a serem processados.")


# =========================================================
# ETAPA 2: BAIXAR, TRATAR E SALVAR INCREMENTALMENTE
# =========================================================
print("\nüîΩ Iniciando processamento incremental...")
total_linhas = 0

for download_url in csv_links:
    # A URL mais recente pode ser longa, pegamos o √∫ltimo segmento para nome de exibi√ß√£o
    filename = download_url.split('/')[-1].split('?')[0] 
    raw_path = os.path.join(RAW_DIR, filename)
    
    print(f"   -> Processando: {filename}")

    try:
        # 1. Download e Leitura
        response = requests.get(download_url, stream=True)
        response.raise_for_status()
        with open(raw_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        # Leitura: Usando encoding="latin-1" para evitar erro de codifica√ß√£o
        df = pd.read_csv(raw_path, sep=";", encoding="latin-1", low_memory=False)
        
        # 2. Tratamento e Renomea√ß√£o (Aplicado a cada DF individual)
        COLUNAS_PARA_RENOMEAR = {
            'OCORRENCIA_ANO': 'ano',                     
            'NATUREZA1_DESCRICAO': 'tipo_crime',       
            'ATENDIMENTO_BAIRRO_NOME': 'bairro',        
            'OCORRENCIA_DATA': 'data_completa'          
        }
        df.rename(columns=COLUNAS_PARA_RENOMEAR, inplace=True)

        if 'data_completa' in df.columns:
            # OTIMIZA√á√ÉO: Define o formato exato para acelerar a convers√£o
            df["data"] = pd.to_datetime(df["data_completa"], errors="coerce", format='%d/%m/%Y %H:%M:%S')
            df["mes"] = df["data"].dt.month

        # 3. Escrita Incremental no Arquivo Final (Otimiza√ß√£o de Mem√≥ria)
        header_flag = primeiro_arquivo # N√£o precisa de 'global' aqui
        mode_flag = 'w' if primeiro_arquivo else 'a'
        
        df.to_csv(
            FINAL_CSV_PATH, 
            mode=mode_flag, 
            header=header_flag, 
            index=False, 
            encoding="utf-8"
        )
        
        total_linhas += len(df)
        print(f"      ‚úÖ Adicionado ({len(df)} linhas). Total acumulado: {total_linhas} linhas.")
        
        primeiro_arquivo = False

    except requests.exceptions.HTTPError as e:
        print(f"      ‚ùå Erro HTTP ao processar {filename}: {e}")
    except Exception as e:
        print(f"      ‚ùå Erro inesperado ao processar {filename}: {e}")

# --- FIM DA COLETA E PROCESSAMENTO ---
print(f"\n‚úÖ Pipeline conclu√≠do. Total de dados coletados e salvos: {total_linhas} linhas.")