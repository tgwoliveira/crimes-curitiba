import pandas as pd
import os
import requests
from bs4 import BeautifulSoup

# --- Configura√ß√µes de URL ---
# Fonte 1: Hist√≥rico (√≠ndice de arquivos)
BASE_URL_HISTORICO = "http://dadosabertos.c3sl.ufpr.br/curitiba/Sigesguarda/"
# Fonte 2: Mais Recente (p√°gina com links em tabela)
URL_DADOS_MAIS_RECENTES = "https://dadosabertos.curitiba.pr.gov.br/conjuntodado/detalhe?chave=b16ead9d-835e-41e8-a4d7-dcc4f2b4b627"
# ----------------------------

# Pasta de sa√≠da (apenas a pasta RAW √© necess√°ria)
RAW_DIR = "data/raw"
os.makedirs(RAW_DIR, exist_ok=True)


def get_historic_links():
    """Busca links de arquivos CSV no portal hist√≥rico."""
    csv_links = []
    try:
        response = requests.get(BASE_URL_HISTORICO)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        for link in soup.find_all('a'):
            href = link.get('href')
            if href and href.endswith('.csv') and 'sigesguarda' in href:
                csv_links.append(BASE_URL_HISTORICO + href)
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erro ao acessar o portal hist√≥rico: {e}")
    return csv_links


def get_latest_links_from_table():
    """Busca links de arquivos CSV na tabela do portal mais recente (2024/2025)."""
    print("üåê Buscando links na tabela do portal de dados mais recentes...")
    recent_links = []
    try:
        response = requests.get(URL_DADOS_MAIS_RECENTES)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Procuramos por links que contenham o padr√£o de nome de arquivo CSV
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if href and 'Base_de_Dados.csv' in href and 'mid-dadosabertos' in href:
                recent_links.append(href)
        
        return recent_links

    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erro ao acessar o portal mais recente: {e}")
        return []


# =========================================================
# ETAPA 1: RASPAR TODAS AS FONTES DE DADOS
# =========================================================

# 1. Coleta Hist√≥rica
csv_links = get_historic_links()
print(f"‚úÖ {len(csv_links)} arquivos CSV encontrados na fonte hist√≥rica (2016-2023).")

# 2. Coleta Recente
recent_links = get_latest_links_from_table()
for link in recent_links:
    if link not in csv_links:
        csv_links.append(link)

print(f"‚úÖ {len(recent_links)} arquivos CSV encontrados na fonte recente (2024/2025).")

if not csv_links:
    print("‚ö†Ô∏è NENHUM link CSV v√°lido encontrado em nenhuma fonte.")
    exit()

# Ordenar a lista para garantir o processamento em ordem cronol√≥gica
csv_links.sort() 
print(f"\n‚úÖ Total de {len(csv_links)} arquivos √∫nicos a serem processados.")


# =========================================================
# ETAPA 2: BAIXAR E SALVAR ARQUIVOS INDIVIDUAIS EM data/raw/
# =========================================================
print("\nüîΩ Iniciando download incremental...")

for download_url in csv_links:
    # Cria um nome de arquivo seguro a partir da URL
    filename = download_url.split('/')[-1].split('?')[0] 
    raw_path = os.path.join(RAW_DIR, filename)
    
    # Se o arquivo j√° existe, pulamos o download
    if os.path.exists(raw_path):
        print(f"   -> Pulando: {filename} (j√° existe)")
        continue
        
    print(f"   -> Baixando: {filename}")

    try:
        # 1. Download
        response = requests.get(download_url, stream=True)
        response.raise_for_status()
        
        # 2. Salvar o arquivo bruto
        with open(raw_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        print(f"      ‚úÖ Salvo em {raw_path}")

    except requests.exceptions.HTTPError as e:
        print(f"      ‚ùå Erro HTTP ao processar {filename}: {e}")
    except Exception as e:
        print(f"      ‚ùå Erro inesperado ao processar {filename}: {e}")

# --- FIM DA COLETA ---
print("\n‚úÖ Pipeline de download conclu√≠do. Arquivos individuais prontos para an√°lise no Notebook.")