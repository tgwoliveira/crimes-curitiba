import pandas as pd
import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime

# --- Configura√ß√µes de URL e Pastas ---
# URL da p√°gina que cont√©m a tabela com os links mais recentes (a que voc√™ enviou)
URL_DADOS_MAIS_RECENTES = "https://dadosabertos.curitiba.pr.gov.br/conjuntodado/detalhe?chave=b16ead9d-835e-41e8-a4d7-dcc4f2b4b627"

PROCESSED_DIR = "data/processed"
FINAL_CSV_PATH = os.path.join(PROCESSED_DIR, "ocorrencias_tratadas.csv")
os.makedirs(PROCESSED_DIR, exist_ok=True)

# Mapeamento de colunas (CRUCIAL para o tratamento)
COLUNAS_PARA_RENOMEAR = {
    'OCORRENCIA_ANO': 'ano',                     
    'NATUREZA1_DESCRICAO': 'tipo_crime',       
    'ATENDIMENTO_BAIRRO_NOME': 'bairro',        
    'OCORRENCIA_DATA': 'data_completa'          
}

# =========================================================
# FUN√á√ïES DE COLETA E PREPARA√á√ÉO
# =========================================================

def get_latest_processed_date():
    """L√™ o CSV final para encontrar a data mais recente processada."""
    if os.path.exists(FINAL_CSV_PATH):
        try:
            # Lendo apenas a coluna 'data' para economizar mem√≥ria
            df_existing = pd.read_csv(
                FINAL_CSV_PATH, 
                usecols=['data'], 
                encoding="utf-8", 
                parse_dates=['data']
            )
            return df_existing['data'].max()
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao ler data do CSV existente: {e}")
            return datetime(2016, 1, 1)
    return datetime(2016, 1, 1)


def get_most_recent_download_link():
    """Raspa a tabela do novo portal e retorna o link do arquivo mais novo (primeiro da lista)."""
    print("üåê Buscando link mais recente na tabela do portal...")
    
    try:
        response = requests.get(URL_DADOS_MAIS_RECENTES)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # CORRE√á√ÉO: Procuramos por todas as tags <a> cujo href contenha "Base_de_Dados.csv"
        # O link mais recente geralmente √© o primeiro item da tabela (ou o √∫ltimo link encontrado)
        
        latest_link = None
        
        # A nova URL de download come√ßa com 'https://mid-dadosabertos.curitiba.pr.gov.br/'
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            # Filtra links que s√£o arquivos CSV de Base de Dados
            if href and 'Base_de_Dados.csv' in href:
                # O loop ir√° encontrar todos, mas a estrutura da tabela HTML coloca o mais novo no topo.
                # Como a tabela HTML lista os arquivos em ordem decrescente de data (2025-06-25 √© o primeiro), 
                # o primeiro link que encontrarmos ser√° o mais novo.
                latest_link = href
                break # Pega o primeiro e sai do loop

        if latest_link:
            return latest_link
        else:
            print("‚ö†Ô∏è Link de Base de Dados mais recente n√£o encontrado na tabela.")
            return None

    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erro ao acessar o portal mais recente: {e}")
        return None


# =========================================================
# FLUXO PRINCIPAL DE ATUALIZA√á√ÉO
# =========================================================

print("üîÑ Iniciando rotina de atualiza√ß√£o de dados...")

# 1. Encontra a data mais recente no arquivo atual
data_limite = get_latest_processed_date()
print(f"Data mais recente no arquivo: {data_limite.strftime('%Y-%m-%d %H:%M:%S')}")


# 2. Coleta o link da fonte mais recente
download_url = get_most_recent_download_link()

if not download_url:
    print("‚ùå Falha ao obter o link de download. Encerrando.")
    exit()

filename = download_url.split('/')[-1]
print(f"   -> Baixando arquivo: {filename}")

try:
    # 3. Baixar e Ler o arquivo mais recente
    response = requests.get(download_url, stream=True)
    response.raise_for_status()
    
    # Lemos o arquivo diretamente do stream (mais eficiente)
    df_new = pd.read_csv(
        response.raw, 
        sep=";", 
        encoding="latin-1", 
        low_memory=False
    )
    
    # 4. Tratamento e Renomea√ß√£o
    df_new.rename(columns=COLUNAS_PARA_RENOMEAR, inplace=True)
    
    # Converte e cria a coluna 'data' com o formato correto
    df_new['data'] = pd.to_datetime(
        df_new["data_completa"], 
        errors="coerce", 
        format='%d/%m/%Y %H:%M:%S'
    )
    df_new['mes'] = df_new['data'].dt.month
    
    # 5. Filtragem (o Cora√ß√£o da atualiza√ß√£o)
    # Filtra apenas registros mais novos que o √∫ltimo registro do seu arquivo final
    df_novos_registros = df_new[df_new['data'] > data_limite]
    
    if df_novos_registros.empty:
        print("‚úÖ Arquivo baixado, mas n√£o h√° novos registros desde a √∫ltima atualiza√ß√£o. Tudo OK.")
        exit()

    # 6. Salvar apenas os novos dados (modo append)
    df_novos_registros.to_csv(
        FINAL_CSV_PATH, 
        mode='a', 
        header=False, 
        index=False, 
        encoding="utf-8"
    )

    print(f"‚úÖ Atualiza√ß√£o conclu√≠da! {len(df_novos_registros)} novos registros adicionados.")

except requests.exceptions.RequestException as e:
    print(f"‚ùå Erro de rede ao baixar o arquivo: {e}")
except Exception as e:
    print(f"‚ùå Erro no processamento do arquivo: {e}")